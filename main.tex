\documentclass[conference]{IEEEtran}


%XXX Remove before final submission
\usepackage{todonotes}

\usepackage{cite}
\usepackage{amsmath}
\usepackage{subcaption}
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
\usepackage{url}
\usepackage{hyperref}

% correct bad hyphenation here
\hyphenation{}


\begin{document}

%XXX remove before submission
\newcommand{\TODO}[1]{\todo[inline]{#1}}
\newcommand{\TODOFIG}[1]{\missingfigure{#1}}

\title{The Statistics of Benchmarks for Testing Perfomance Regressions}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Jiahao Chen and Jarrett Revels}
\IEEEauthorblockA{Computer Science and Artificial Intelligence Laboratory\\
Massachusetts Institute of Technology\\
Cambridge, Massachusetts 02139--4307\\
Email: \{jiahao,jrevels\}@csail.mit.edu}
}

% make the title area
\maketitle

\begin{abstract}
\TODO{The abstract goes here.}
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

Developers who care about high performance computing need not only unit tests that
ensure that their programs are correct, but also regression tests that safeguard
against performance degradation. However, modern computers are extremely complicated
systems that do many things under the hood~\cite{HP5e}.
For example, modern CPUs feature frequency scaling, thread rescheduling,
out of order execution,
and variations in memory layout (ASLR, swapping, etc.), the exact NUMA architecture
of which may lead to effects like cache misses.
Furthermore the operating system may introduce further performance variations
due to processes like paging virtual memory.
And then there is environmental factors such as network traffic that may lead to
further variation. We call all these factors jitter.
Jitter complicates the accurate and reproducible timing of programs, which we
need so that we can say that ``this change made my program run faster or slower''.

Figure~\ref{fig:examplebenchmarks} illustrates some of the many behaviors exhibited by benchmark codes.

\begin{figure}
\centering

\begin{subfigure}{0.22\textwidth}
    \centering
    \includegraphics[width=\textwidth]{experiments/images/mean_repeated_branchsum}
    \caption{TODO Simple variation}
\end{subfigure}%
~
\begin{subfigure}{0.22\textwidth}
    \centering
    \includegraphics[width=\textwidth]{experiments/images/mean_repeated_branchsum}
    \caption{Bimodal distribution}
\end{subfigure}

\begin{subfigure}{0.22\textwidth}
    \centering
    \includegraphics[width=\textwidth]{experiments/images/mean_repeated_sqralloc}
    \caption{Systematic drift}
\end{subfigure}
~
\begin{subfigure}{0.22\textwidth}
    \centering
    \includegraphics[width=\textwidth]{experiments/images/mean_repeated_sqralloc}
    \caption{TODO More crazy?}
\end{subfigure}

%\includegraphics[width=2.5in]{myfigure}
\caption{Some typical variations in benchmark timings.}
\label{fig:examplebenchmarks}
\end{figure}

There is a lot of work on machine quiescence, to reduce variation in the performance
behavior of CPUS. Relevant work exists to

- reducing OS jitter to increase data reproducibility

- reducing OS jitter to increase performance

- randomization of  to induce Gaussian timing distributions

- strategies for orchestrating performance counters

Nevertheless it is impossible to eliminate variation
entirely, and one may even argue that quiescence is not a typical machine
configuration that users encounter in practice.
In these cases where there is irreducible jitter we have no choice but to resort to a statistical description of the benchmarking process.

In this paper, we present a statistically rigorous approach to performance regression
testing that is valid in the presence of OS jitter.

\TODO{portability}



\section{Methodology}

We only focus on stateless benchmarks $P_0$, so that in principle, the benchmark program can be run many times, each time producing the same deterministic output. Even though the benchmark program $P_0$ itself may be stateless, its environment---the hardware it runs on, the operating system, and other programs that may also be running on the machine, for example---does carry some state and produces side effects which introduce variations in the runtime of $P_0$.

In order to classify regressions in the presence of these variations, we need to develop a rigorous statistical model for our runtime that incorporates them.

\label{sec:statmodel}
\subsection{A simple statistical model for nondeterministic execution times in the presence of OS jitter}

In this work we adopt a simple model for program execution. Assume for the moment that we have only a single instruction pipeline and that the user's program $P_0$ may be specified as a single sequence of instructions, which we shall denote

$P_0$ = \begin{tabular}{|c|c|c|c|c|}
\hline
$I_1$ & $I_2$ & $\cdots$ & $I_N$ \tabularnewline
\hline
\end{tabular}

Assume that each instruction $I_i$ takes exactly time $T_{I_i}$ to execute.
In an idealized universe, the user program $P_0$ will execute in time $T_{P_0} = \sum_{i=1}^N T_{I_i}$ clock cycles. However, due to any of the external factors mentioned above, the CPU or operating system may choose to do things other than one of the instructions $I_k$. We model this by saying that the idealized program $P_0$ is transformed into a new program $P$, which consists of the same sequence of instructions from the user's program $P_0$, but interleaved with delay instructions $D_j$:

$P$ = \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$D_1$ & $I_1$ & $D_2$ & $I_2$ &
$\cdots$ & $D_N$ & $I_N$ & $D_{N+1}$
\tabularnewline
\hline
\end{tabular}

The delay instructions model branching to instructions that do not belong to $P_0$, but are nevertheless run because they are triggered by the environment of $P_0$, such as its operating system or hardware state, or even a network interrupt. The run time of $P$ is no longer $T_{P_0}$, but
$T_P = T_{P_0} + \sum_{j=1}^{N+1} D_j$ instead.

Further imagine that this machine has $N_f$ independent factors of OS (environmental) jitter, which we label in superscripts $\cdot^{(i)}$. Each jitter factor can trigger with probability $p^{(i)}$, and each time the jitter factor is triggered, it will take time $T^{(i)}$ to run. Let $x^{i}_j \in \{0, 1\}$ indicate whether the $i$th jitter factor is triggered each time in the random delay instruction $R_j$. Then the run time of $D_j$ is
$T_{D_j} = \sum_{i=1}^{N_f} x^{(i)}_j T^{(i)}$ and is no longer deterministic, but instead a random variable.

Putting all these things together, we get

\begin{equation}
T_P = T_{P_0} + \sum_{j=1}^{N+1} \sum_{i=1}^{N_f} x^{(i)}_j T^{(i)}
= T_{P_0} + \sum_{i=1}^{N_f} X_P^{(i)} T^{(i)},
\end{equation}
%
where $X_P^{(i)} = \sum_{j=1}^{N+1} x^{(i)}_j$ is the number of times the jitter factor $i$ was triggered when running the program $P$.

The randomness has a natural interpretation in the Kolmogorov formulation of probability. We could in principle gather so much information about the exact microscopic configuration $\mathcal S$ of a computer, down to its precise hardware specification, operating temperature, and all the bits in its I/O streams, caches, buffers, and storage devices, and so on, that we could know deterministically how long each delay instruction $D_j$ would take to run. If we could also restore the computer to the precise state $\mathcal S$ after running the program $P$, then the computer will execute $P$ again with exactly the same, deterministic run time. In practice, however, we never know $\mathcal S$ with enough detail to describe $T_P$ deterministically. Instead, we must make do with an imprecise specification of the machine state, saying instead that the machine state $\mathcal S$ is only known to belong to some set $S$ of possible machine states belonging to the sample space $\Omega$. In this sense, our probabilistic description of program execution is reminiscent of the microcanonical ensemble in statistical mechanics.

Let's note a few properties of this model:

First of all, $T_P \ge T_{P_0}$ since each characteristic time $T^{(i)} > 0$ is positive. Therefore $P$ always takes more time to execute than $P_0$.
For this model to apply, we must assume that jitter that causes \textit{faster}
execution times, such as frequency boosting, are absent and disabled with the
appropriate hardware and OS configuration settings. We have equality $T_P = T_{P_0}$
for machine states $\mathcal S_0$ that trigger no jitter whatsoever. Such a machine state may not exist, but nevertheless there must exist some state (or collection of states) $\mathcal S_*$ that minimizes $T_P$ to some value, $T^*_P$, invoking the fewest possible jitter factors that lead to smallest possible increase in execution time over $T_{P_0}$.
Statistically speaking, there must exist some critical time $T^*_P$ for which the probability density function (pdf) $f_P(t)$ of $T_P$ of obeys $f_P(t) = 0 \; \forall \; t < T^*_P$.

Secondly, there are special case limiting behaviors that make a lot of sense. For example, if there is only $N_f = 1$ factor of OS jitter, then $X^{(1)}$ follows a binomial distribution with $N$ samples and proability $p^{(1)}$. If furthermore the number of instructions $N$ is sufficiently large, then and there is only $N_f = 1$ factor of OS jitter, then the behavior of $X^{(1)}$ follows a Poisson (exponential) distribution with rate $p^{(1)} N$. If there are $N_f > 1$ factors of OS jitter, each with identical trigger probability $p^{(i)} = \pi$ and characteristic time scale $T^(i) = \tau$, then $X = \sum_{i=1}^{N_f} X^{(i)}$ follows an Erlang distribution with shape parameter $k$ and rate $p^{(1)} N$. Of course in practice, few benchmarks obey such idealized behavior, but they are useful limiting cases to bear in mind. Furthermore our model is general enough to cover the wide variety of distributions seen in practice. An important nonideal case is the observation of two or more modes in $f_P$. Such behavior is consistent with a model $T_P = T_Y Y + T_Z Z$, where $T_Y$ and $T_Z \ne T_Y$ are constants describing a characteristic time scales associated with random variables $Y$ and $Z$ respectively, and $Y$ is an Erlang distribution and $Z$ is a binomial distribution.



\subsection{A compromise between timer accuracy and jitter determines how many times to run the benchmark}

For benchmarks that take very little time to run, it is vital to run it many times so that you avoid measurement error coming from finite timer accuracy. We distinguish between precision and accuracy here: just because the system may provide a timer with nanosecond precision does not automatically guarantee that it has nanosecond \textit{accuracy}. In principle we can calibrate the timer for a given system to measure its accuracy. In practice, we simply assume a given accuracy, say 1 microsecond, and proceed accordingly.

A common rule of thumb used in practice is to repeat the benchmark enough times, say $n$, so that the total execution time exceeds a certain minimum period, say 5--20 seconds. The relative accuracy of assessing the actual run time is increased. A useful analogy is how you can get a more precise measurement of the weight of a single sheet of paper if you weigh 500 sheets on scale and divide the measurement by 500, as opposed to weighing just a single sheet directly.

Nominally it looks like the relative accuracy of this process should be 1 μs/10s = $10^{-7}$. However, the longer you run the program the more OS jitter you will pick up in the timing. Different benchmarks pick up different jitter. For example memory heavy benchmarks may pick up more cache misses while I/O heavy benchmarks may pick up more hard disk latency. So in practice you want to run the benchmark long enough to minimize timer accuracy error while not pick up too much jitter, and a hard rule of thumb like 5--20 seconds may be too much or not enough.

Some papers
\TODO{who?}
advocate a linear search methodology to choose the optimal $n$ where you keep increasing the number of repetitions linearly up to a fixed upper bound, say 1 minute, then do a linear regression to find which region starts to exhibit linear scaling with the number of repetitions. However, a common error in these descriptions is that they tend to also include many data points with total times comparable or less than the timer accuracy. Actually we should throw all of these away because having many inaccurate points biases the least squares search. Or maybe we should do instead weighted least squares so that the regression knows something about error bars. In practice throwing away the short runs suffices. Also you don't want to have too many data points with very long run times because you may start to trigger new jitter factors with longer characteristic time scales.

Let's justify the linear search strategy. Let $h$ be the timer accuracy, $n_1, n_2, ..., n_k$ be a sequence for the number of repetitions $n$ to be tried, and $T_j$ be the execution time measured when the number of repetitions is $n_j$. We assume that the measured execution time can be decomposed into

\begin{equation}
    T_j(n_j) = n_j \tau + h + \sum_{i=1}^{N_f} X_j^{(i)} T^{(i)}
\end{equation}
%
where $\tau$ is the true execution time for the benchmark. If the last two terms
are constant then a linear regression will have slope $\tau$ and intercept $h + \sum_{i=1}^{N_f} X_P^{(i)} T^{(i)}$. But $h$ is not deterministic. Furthermore, the jitter term will also grow with $n_j$ and this complicates the analysis.
However, if we assume that $h$ is independent of $n_j$ and that each $X_j^{(i)} = o(n_j)$,\footnote{This assumption can be relaxed somewhat to $X_j^{(i)} = O(n_j)$. By definition, there then exist positive constants $M^{(i)} > 0$ such that for all sufficiently large $n_j$, $X_j^{(i)} < M^{(i)} n_j$, in which case the observed gradient is $\tau + \sum_i M^{(i)} > \tau$ } then for sufficiently large $n_j$ the observed gradient is $\tau$.

In practice, we observe that there are some benchmarks that pick up jitter factors with very large time scales. These occurrences can be explained in several ways in our model. The simplest way is to posit some factor $l$ for which $T^{(l)} = O(n_j \tau) = O(n_j)$ and $X_j^{(i)}(n) = H(n - n^\ddagger)$,
where $H$ is the Heaviside step function. The existence of such a factor produces a discontinuity in the gradient at $n^\ddagger$. In practice, some caution must be taken to avoid triggering such a jitter factor.

We therefore recommend a truncated linear regression trimming all runs lasting less than the timer accuracy of 1 μs.
\TODO{This is not the correct description of what we actually do}


\subsection{A hypothesis testing approach to regression testing}

We are now ready to make use of all this probablisitic machinery to describe the statistics of regression benchmarking. Suppose we have a reference program $P$ and a modified program $Q$ which produce the same output when given identical inputs. We now want to know if $P$ is slower than $Q$. Again in a perfectly deterministic universe where there are no OS jitter factors or the starting machine state $\mathcal S$ known with great precision and can be reproduced on demand, then we simply run $P$, time how long it takes, reset the machine back to its starting state $\mathcal S$, and then run $Q$, noting how long it takes to run. However $\mathcal S$ is never known exactly, and even if it were known, it's not clear if
$\mathcal S$ represents a ``typical'' machine configuration encountered in practice.

We are thus inexorably led to consider instead the statistical question of whether

\begin{equation}
\Delta T = T_Q - T_P
= (T^*_P - T^*_Q) + \sum_{i=1}^{N_f} (X^{(i)}_P - X^{(i)}_Q) T^{(i)}
\end{equation}
%
is statistically positive.

What we need to do now is to distinguish the constant factor $T^*_P - T^*_Q$, the part we care about, from the second term, representing differences in OS jitter triggers.

\TODO{... SKIP SOME STEPS ...}

Let $l_P$ and $l_Q$ be location parameters, such as the means, medians, minima, or interquartile ranges that characterize $f_P$ and $f_Q$. Then we can write down a hypothesis test

\begin{subequations}
\begin{align}
H_0&: l_P = l_Q \\
H_1&: l_P < l_Q
\end{align}
\end{subequations}

If $H_1$ is true, then we have a performance regression if program $Q$ replaces program $P$.

There is a question of what location parameter to use.
One may want to use the means, but as we all know the means are sensitive to large outliers. We see large outliers in the benchmark data, which happens when some OS jitter factor with a very long delay is triggered. Quantile-based measures such as the mean and minimum are sensitive to non-stationary OS jitter, for example if there is some jitter factor $i$ such that $X^{(i)}_P = 1$ while $X^{(i)}_Q = 0$ determistically. There is a fundamental definition problem in this situation. If we acknowledge that factor $i$ is always triggered once every time program $P$ runs but not in $Q$, then we could could fold the contribution of factor $i$ into the definition of $T^*_P$, recognizing that it is not possible to run $P$ without triggering factor $i$. However, it is impossible to distinguish statistically between this case and non-stationary jitter, i.e.\ jitter that changes the machine's behavior for a time period longer than, say, the combined run times of $P$ and $Q$. For example, the OS may decide to start paging virtual memory to disk halfway through running $P$, which continues all throughout the time when the user runs $Q$. Another possibility is that the machine running the benchmark encounters an unusally high level of network traffic when running $P$ and $Q$, for example if the machine is suffering a DDoS attack. The only way to detect this kind of nonstationary noise is to run $P$ and $Q$ enough times so that the combined run time exceeds the characteristic time scale for this jitter. However, in practice this may require benchmarks to run for unacceptably long times.

Therefore our current recommendation is to use an inter-quantile range as the location parameter. It is easy to calculate the estimators $\hat l_P$ and $\hat l_Q$. However, we don't know analytically what the distribution of $l_P - l_Q$ looks like, which we need to know in order to describe the appropriate time scale $\tau$ to describe statistical significance. (This time scale generalizes the standard error of the $t$-statistic when comparing the means of two Gaussians.) So we use a bootstrap procedure to estimate this scale $\tau$~\cite{Efron1981}.



\subsection{A bootstrap procedure for hypothesis testing}

\TODO{jiahao to fill in}




\section{Implementation in Julia}

We have implemented the stastistical ideas described above in a suite of packages available on GitHub under the \href{https://github.com/JuliaCI}{JuliaCI} organization.
The code is implemented in Julia. Julia is an emerging programming language designed for technical computing, an area where performance considerations are very important.
The code is available freely on GitHub under the MIT ``Expat'' License.

\begin{itemize}

\item
\href{https://github.com/JuliaCI/BenchmarkTools.jl}{BenchmarkTools.jl}
is a Julia package for automated benchmarking and regression testing, implementing the statistical ideas described above for automated regression testing.

\item
\href{https://github.com/JuliaCI/BaseBenchmarks.jl}{BaseBenchmarks.jl}
is a collection of approximately 2,500 benchmarks which can be used for testing performance regressions in the Julia base library and compiler.

\item
\href{https://github.com/JuliaCI/Nanosoldier.jl}{Nanosoldier.jl}
contains GitHub API hooks that trigger some or all the benchmarks to be run on the Nanosoldier, a dedicated machine at MIT reserved solely for regression testing.
Julia developers can simply write a suitable comment on issues and pull requests on the Julia repository on GitHub that mention the \href{https://github.com/nanosoldier}{@nanosoldier} GitHub user. The bot underneath runs the regression tests on the Nanosoldier, commits a report to the \href{https://github.com/JuliaCI/BaseBenchmarkReports}{BaseBenchmarkReports} repository, and replies in a follow-up comment with a summary indicating any potential performance regressions detected.

\end{itemize}



\section{Results on some Julia benchmarks}

Samples generally appears to follow a Gamma distribution, plus modes at points of high-frequency noise (large `xᵢ`s). The modality/phase structure of the samples seems to be a property inherent to each benchmark, . This could be interpreted as either the benchmark is characteristically vulnerable to certain sources of noise, or that the benchmark triggers certain sources of noise with high frequency.


\begin{figure}[!t]
\centering
\TODOFIG{Some clusters}
%\includegraphics[width=2.5in]{myfigure}
\caption{Clustering of Julia benchmarks.}
\label{fig:benchclusters}
\end{figure}

\begin{figure}[!t]
\centering
\TODOFIG{Real benchmark timings}
%\includegraphics[width=2.5in]{myfigure}
\caption{How benchmark timings scale with the number of repetitions $n$.}
\label{fig:scaling}
\end{figure}


\section{Limitations of our current approach}



\section{Conclusion}

\TODO{The conclusion goes here.}



\section*{Acknowledgment}

We thank the many Julia developers for many insightful discussions.

This work was supported by the Nanosoldier grant.\TODO{Get grant info}

\bibliography{biblio}
\bibliographystyle{IEEEtran}

% that's all folks
\end{document}
\grid
